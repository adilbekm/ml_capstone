{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN with image augmentation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I use image augmentation with the CNN from Model 1.\n",
    "\n",
    "Predictions made using this model scored 0.276 by Kaggle, slightly worse than the 1st model result of 0.283 (submitted on Jan 18, 2019).\n",
    "\n",
    "Hardware used: CPU: i5 2.10GHz x 6, GPU: none: RAM: 16Gb + 32Gb virtual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Conv2D, AveragePooling2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image         Id\n",
      "0  0000e88ab.jpg  w_f48451c\n",
      "1  0001f9222.jpg  w_c3d896a\n",
      "2  00029d126.jpg  w_20df2c5\n",
      "3  00050a15a.jpg  new_whale\n",
      "4  0005c1ef8.jpg  new_whale\n",
      "25361\n"
     ]
    }
   ],
   "source": [
    "# load train files and labels into dataframe\n",
    "traindf_all = pd.read_csv('train.csv')\n",
    "print(traindf_all.head())\n",
    "print(len(traindf_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15697\n"
     ]
    }
   ],
   "source": [
    "# remove unlabeled images\n",
    "traindf = traindf_all.drop(traindf_all[traindf_all.Id == 'new_whale'].index.tolist())\n",
    "traindf.reset_index(drop=True, inplace=True)\n",
    "del traindf_all\n",
    "print(len(traindf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Count\n",
      "0  w_f48451c     14\n",
      "1  w_c3d896a      4\n",
      "2  w_20df2c5      4\n",
      "5004\n"
     ]
    }
   ],
   "source": [
    "# create dataframe with distinct ids and count of images per id\n",
    "ids = pd.DataFrame(traindf['Id'].unique(), columns=['Id'])\n",
    "ids['Count'] = 0\n",
    "for r in ids.itertuples():\n",
    "    id = r.Id\n",
    "    cnt = len(traindf[traindf['Id'] == id])\n",
    "    ind = ids[ids['Id'] == id].index.values[0]\n",
    "    ids.loc[ind, 'Count'] = cnt\n",
    "print(ids.head(3))\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image         Id  Width  Height Mode\n",
      "0  0000e88ab.jpg  w_f48451c   1050     700  RGB\n",
      "1  0001f9222.jpg  w_c3d896a    758     325  RGB\n",
      "2  00029d126.jpg  w_20df2c5   1050     497  RGB\n",
      "3  000a6daec.jpg  w_dd88965   1050     458  RGB\n",
      "4  0016b897a.jpg  w_64404ac   1050     450  RGB\n"
     ]
    }
   ],
   "source": [
    "# Get image dimensions and color mode for all training images\n",
    "traindf['Width'] = 0\n",
    "traindf['Height'] = 0\n",
    "traindf['Mode'] = ''\n",
    "i = 0\n",
    "for r in traindf.itertuples(): \n",
    "    img_name = r.Image \n",
    "    img_path = 'train/' + img_name\n",
    "    img = Image.open(img_path) \n",
    "    width, height = img.size\n",
    "    mode = img.mode\n",
    "    traindf.loc[i, ['Width', 'Height', 'Mode']] = width, height, mode\n",
    "    i += 1\n",
    "print(traindf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Using augmented images ###\n",
    "\n",
    "In the following cells, I proceed with the dataset <code>subset</code> that uses augmented images in addition to existing ones. This dataset has exactly 5 images per whale Id (25,020 total images) made of some combination of existing and new images obtained using random augmentation. In creating this dataset, I used augmented images where an Id had fewer than 5 images, and I dropped all but 5 of the existing images where an Id had more than 5 images.\n",
    "\n",
    "See section **\"Implementing Image Augmentation\"** at the end of this notebook for code and other details. Briefly, I created 484,703 new images for a total of 500,400 old *and* new images, resulting in exactly 100 images per Id.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image         Id  New\n",
      "0  0000e88ab.jpg  w_f48451c    0\n",
      "1  0001f9222.jpg  w_c3d896a    0\n",
      "2  00029d126.jpg  w_20df2c5    0\n",
      "3  000a6daec.jpg  w_dd88965    0\n",
      "4  0016b897a.jpg  w_64404ac    0\n",
      "25020\n"
     ]
    }
   ],
   "source": [
    "# load subset\n",
    "subset = pd.read_csv('subset.csv')\n",
    "print(subset.head())\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert images to tensors\n",
    "def imgs_to_tensors(df, path, size=(100, 100)):\n",
    "    '''\n",
    "    df: dataframe listing image file names in column \"Image\"\n",
    "    path: directory where image files are located (don't include /)\n",
    "    size: target height and width to resize images to\n",
    "    '''\n",
    "    HEIGHT, WIDTH = size\n",
    "    LEN=df.shape[0]   \n",
    "    tensors = np.zeros((LEN, HEIGHT, WIDTH, 3))\n",
    "    i = 0\n",
    "    for im_name in df.Image:\n",
    "        if (i%1000==0):\n",
    "            print('Processing image {}: {}'.format(i, im_name))\n",
    "        im_path = path + '/' + im_name\n",
    "        # load image to PIL format\n",
    "        im = image.load_img(path=im_path, \n",
    "                            grayscale=False, \n",
    "                            color_mode='rgb', \n",
    "                            target_size=(HEIGHT, WIDTH), \n",
    "                            interpolation='nearest')\n",
    "        # convert to numpy array/tensor with shape (HEIGHT, WIDTH, 3)\n",
    "        x = image.img_to_array(im)\n",
    "        x = preprocess_input(x) # important line! I am not sure why\n",
    "        tensors[i] = x\n",
    "        i += 1   \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors of training images and save on disk\n",
    "# (divide by 255 to normalize pixel values)\n",
    "tensors_train = imgs_to_tensors(df=subset, path='augmented/train')/255\n",
    "np.save('tensors/model_2/tensors_train', tensors_train)\n",
    "print(tensors_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(ids.Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels of training images and save on disk\n",
    "tensors_train_labels = np.zeros((len(subset), len(ids)))\n",
    "i = 0\n",
    "for id in subset.Id:\n",
    "    j = np.argwhere(labels==id)[0, 0]\n",
    "    tensors_train_labels[i, j] = 1\n",
    "    i += 1\n",
    "np.save('tensors/model_2/tensors_train_labels', tensors_train_labels)\n",
    "print(tensors_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved tensors and labels, if any\n",
    "tensors_train = np.load('tensors/model_2/tensors_train.npy')\n",
    "tensors_train_labels = np.load('tensors/model_2/tensors_train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               4608500   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5004)              2507004   \n",
      "=================================================================\n",
      "Total params: 7,126,048\n",
      "Trainable params: 7,126,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build basic model\n",
    "# (similar to one described in Lesson 2.18 in Deep Learning section of ML Nanodegree)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu',\n",
    "                       input_shape=(tensors_train.shape[1], tensors_train.shape[2], 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(tensors_train_labels.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22518 samples, validate on 2502 samples\n",
      "Epoch 1/10\n",
      "22518/22518 [==============================] - 134s 6ms/step - loss: 8.5483 - acc: 8.8818e-05 - val_loss: 8.9617 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.96174, saving model to saved_models/weights.model_2.h5\n",
      "Epoch 2/10\n",
      "22518/22518 [==============================] - 135s 6ms/step - loss: 8.4741 - acc: 8.8818e-05 - val_loss: 9.3614 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 8.96174\n",
      "Epoch 3/10\n",
      "22518/22518 [==============================] - 137s 6ms/step - loss: 8.2363 - acc: 4.8850e-04 - val_loss: 10.3818 - val_acc: 3.9968e-04\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 8.96174\n",
      "Epoch 4/10\n",
      "22518/22518 [==============================] - 140s 6ms/step - loss: 7.8193 - acc: 0.0027 - val_loss: 11.1784 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 8.96174\n",
      "Epoch 5/10\n",
      "22518/22518 [==============================] - 137s 6ms/step - loss: 7.2607 - acc: 0.0115 - val_loss: 12.2023 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 8.96174\n",
      "Epoch 6/10\n",
      "22518/22518 [==============================] - 138s 6ms/step - loss: 6.5992 - acc: 0.0333 - val_loss: 13.0459 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 8.96174\n",
      "Epoch 7/10\n",
      "22518/22518 [==============================] - 138s 6ms/step - loss: 5.8697 - acc: 0.0723 - val_loss: 13.6239 - val_acc: 3.9968e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 8.96174\n",
      "Epoch 8/10\n",
      "22518/22518 [==============================] - 138s 6ms/step - loss: 5.1085 - acc: 0.1371 - val_loss: 13.8806 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 8.96174\n",
      "Epoch 9/10\n",
      "22518/22518 [==============================] - 138s 6ms/step - loss: 4.3432 - acc: 0.2194 - val_loss: 14.1255 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 8.96174\n",
      "Epoch 10/10\n",
      "22518/22518 [==============================] - 139s 6ms/step - loss: 3.6149 - acc: 0.3146 - val_loss: 14.4089 - val_acc: 0.0012\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 8.96174\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.model_2.h5', verbose=1, save_best_only=True)\n",
    "history = model.fit(\n",
    "        x=tensors_train,\n",
    "        y=tensors_train_labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpointer],\n",
    "        validation_split=0.1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model.load_weights('saved_models/weights.model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image\n",
      "0  21253f840.jpg\n",
      "1  769f8d32b.jpg\n",
      "2  a69dc856e.jpg\n",
      "7960\n"
     ]
    }
   ],
   "source": [
    "# load test files into dataframe\n",
    "filelist = os.listdir('test')\n",
    "testdf = pd.DataFrame(filelist, columns=['Image'])\n",
    "print(testdf.head(3))\n",
    "print(len(testdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors for test images and save on disk\n",
    "tensors_test = imgs_to_tensors(df=testdf, path='test')/255\n",
    "np.save('tensors/model_2/tensors_test', tensors_test)\n",
    "print(tensors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved test tensors, if any\n",
    "tensors_test = np.load('tensors/model_2/tensors_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7960/7960 [==============================] - 9s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = model.predict(tensors_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image                                                 Id\n",
      "0  21253f840.jpg  new_whale w_022b708 w_4ba728f w_686c0b3 w_aa3489d\n",
      "1  769f8d32b.jpg  new_whale w_686c0b3 w_022b708 w_aa3489d w_4ba728f\n",
      "2  a69dc856e.jpg  new_whale w_022b708 w_4ba728f w_686c0b3 w_aa3489d\n",
      "3  79bee536e.jpg  new_whale w_022b708 w_686c0b3 w_4ba728f w_aa3489d\n",
      "4  7eb9a6f1b.jpg  new_whale w_022b708 w_4ba728f w_686c0b3 w_aa3489d\n",
      "5  8e0a9e74b.jpg  new_whale w_686c0b3 w_022b708 w_c06798f w_a6067a9\n",
      "6  4853537ad.jpg  new_whale w_022b708 w_4ba728f w_686c0b3 w_aa3489d\n",
      "7  8cba4a867.jpg  new_whale w_686c0b3 w_022b708 w_aa3489d w_4ba728f\n",
      "8  8da08a11a.jpg  new_whale w_022b708 w_686c0b3 w_4ba728f w_aa3489d\n",
      "9  48a937823.jpg  new_whale w_022b708 w_686c0b3 w_4ba728f w_aa3489d\n"
     ]
    }
   ],
   "source": [
    "# get 5 best predictions per image and decode to whale ids\n",
    "# insert \"new_whale\" where prediction probability drops below 10% \n",
    "testdf['Id'] = ''\n",
    "for i, pred in enumerate(predictions):\n",
    "    inx = np.argsort(pred)[-5:][::-1].tolist()\n",
    "    preds = labels[inx].tolist()\n",
    "    probs = pred[inx]\n",
    "    try:\n",
    "        # get index of first prediction with prob less than 10%\n",
    "        j = (probs < 0.1).tolist().index(True)\n",
    "        # enter \"new_whale\" in that index, and shift any remaining preds to right\n",
    "        for ii in range(4, (j-1), -1):\n",
    "            if ii==j:\n",
    "                preds[ii] = 'new_whale'\n",
    "            else:\n",
    "                preds[ii] = preds[ii-1]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    testdf.loc[i,'Id'] = ' '.join(preds)\n",
    "print(testdf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file and submit to Kaggle\n",
    "testdf.to_csv('submissions/submit_0118_04.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scored 0.276 in Kaggle, slightly worse than the Model 1 result of 0.281."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Implementing Image Augmentation ###\n",
    "\n",
    "This section contains my code for creating new images using randomized image augmentation. My goal was to have 100 images (old and new) for each class. So, if a class had only 1 image, I created 99 new images. If it had 10 images, I created 90, and so on. When creating new images, I used all existing images for the class equally (or as equally as possible).\n",
    "\n",
    "**Note** that running the next few cells takes very long (over 20 hours). But it only needs to be done once because the results are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of new images to create/augment per existing image\n",
    "\n",
    "n = 100 # target number of total (original + augmented) imgs\n",
    "traindf['Augment'] = 0\n",
    "\n",
    "for r in ids.itertuples():\n",
    "    id = r.Id\n",
    "    cnt = r.Count\n",
    "    aug_per_img = (100-cnt) // cnt\n",
    "    indx = traindf[traindf['Id'] == id].index.tolist()\n",
    "    if aug_per_img:\n",
    "        traindf.loc[indx, 'Augment'] = aug_per_img\n",
    "    total = cnt*aug_per_img + cnt\n",
    "    short = 100-total\n",
    "    if short:\n",
    "        indx_add = np.random.choice(indx, size=short, replace=False).tolist()\n",
    "        traindf.loc[indx_add, 'Augment'] = aug_per_img + 1\n",
    "\n",
    "print(traindf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new images\n",
    "# (careful, this cell will run about 20 hrs)\n",
    "\n",
    "stop\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=20,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# create new dataframe to hold results\n",
    "traindf_big = pd.DataFrame(index=range(0,500400), columns=['Image','Id','Test','Width','Height','Mode','New'])\n",
    "traindf_big.Test = 0\n",
    "traindf_big.Width = 0\n",
    "traindf_big.Height = 0\n",
    "traindf_big.New = 0\n",
    "\n",
    "print('please wait...')\n",
    "cnt_new = 0\n",
    "cnt_old = 0\n",
    "i = 0\n",
    "for r in traindf.itertuples():\n",
    "    cnt_old += 1\n",
    "    if cnt_old%1000 == 0:\n",
    "        print(cnt_old)\n",
    "    im = r.Image\n",
    "    id = r.Id\n",
    "    wi = r.Width\n",
    "    he = r.Height\n",
    "    mo = r.Mode\n",
    "    au = r.Augment\n",
    "    traindf_big.loc[i,'Image'] = im\n",
    "    traindf_big.loc[i,'Id'] = id\n",
    "    traindf_big.loc[i,'Width'] = wi\n",
    "    traindf_big.loc[i,'Height'] = he\n",
    "    traindf_big.loc[i,'Mode'] = mo\n",
    "    i += 1\n",
    "    if au:\n",
    "        im_path = 'train/' + im\n",
    "        img = Image.open(im_path)\n",
    "        x = image.img_to_array(img)  # this is a Numpy array with shape (3, width, weight)\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, width, height)\n",
    "        j = 1 # not a typo\n",
    "        # generate batches of randomly transformed images and save on disk\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir='preprocessed/temp', save_format='jpeg'):\n",
    "            cnt_new += 1\n",
    "            # move file and give new name\n",
    "            temf = os.listdir('augmented/temp/')[0]\n",
    "            temf_path = 'augmented/temp/' + temf\n",
    "            newf = 'aug{:0>6}.jpg'.format(cnt_new)\n",
    "            newf_path = 'augmented/train/' + newf\n",
    "            os.replace(temf_path, newf_path)\n",
    "            traindf_big.loc[i,'Image'] = newf\n",
    "            traindf_big.loc[i,'Id'] = id\n",
    "            traindf_big.loc[i,'Width'] = wi\n",
    "            traindf_big.loc[i,'Height'] = he\n",
    "            traindf_big.loc[i,'Mode'] = mo\n",
    "            traindf_big.loc[i,'New'] = 1\n",
    "            # print('created a new image ' + str(j) + ' ' + newf)\n",
    "            i += 1\n",
    "            j += 1\n",
    "            if j > au:\n",
    "                break\n",
    "\n",
    "# save the dataframe to file\n",
    "traindf_big.to_csv('train_big.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved dataframe, if any\n",
    "traindf_big = pd.read_csv('train_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see an example of augmented image, along with the existing image it was based on\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "img_old = Image.open('train/9ab65fac4.jpg')\n",
    "img_new = Image.open('augmented/train/aug295753.jpg')\n",
    "ax[0].imshow(img_old)\n",
    "ax[1].imshow(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 5 imgs per label for training into new dataframe\n",
    "# select imgs randomly, but prefer old imgs to new/augmented\n",
    "# expect 5004 * 5 = 25020 rows\n",
    "\n",
    "traindf_big['Subset'] = 0\n",
    "\n",
    "for r in ids.itertuples():\n",
    "    id = r.Id\n",
    "    cnt = r.Count\n",
    "    inx = traindf_big[(traindf_big.Id == id) & (traindf_big.New == 0)].index.tolist()\n",
    "    inx = random.sample(inx, min(len(inx), 5)) \n",
    "    inx_new = traindf_big[(traindf_big.Id == id) & (traindf_big.New == 1)].index.tolist()\n",
    "    inx_new = random.sample(inx_new, 4)\n",
    "    inx.extend(inx_new) # index of imgs for given Id, with old imgs listed first\n",
    "    inx_5 = inx[:5]\n",
    "    traindf_big.loc[inx_5, 'Subset'] = 1\n",
    "\n",
    "subset = traindf_big[traindf_big.Subset==1].copy()\n",
    "subset.drop(['Test', 'Width', 'Height', 'Mode', 'Subset'], axis=1, inplace=True)\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.to_csv('subset.csv', index=False)\n",
    "del traindf_big\n",
    "del subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved dataframe, if any\n",
    "subset = pd.read_csv('subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
